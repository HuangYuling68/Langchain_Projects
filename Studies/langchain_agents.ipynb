{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3f8ed4",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "064653d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cef1bb",
   "metadata": {},
   "source": [
    "## LCEL (LangChain Expression Language)\n",
    "- LangChain 重写了 `|`（`__or__`），`Chain 之所在`\\n\",\n",
    "- `RunnablePassthrough`: RunnablePassthrough 允许你将输入数据直接传递而不做任何更改（identity），通常与 RunnableParallel 一起使用，将数据传递到新的键中。\\n\",\n",
    "- LangChain 的应用 RunnablePassthrough 作为一个占位符，可以在需要时填充数据，比如在公司名称尚未确定时先留空，后续再填入。\\n\",\n",
    "- 所谓的最佳实践\\n\",\n",
    "    - python：遍历，也可以用 list comprehension\\n\",\n",
    "    - 对于 matlab：也可以遍历，也可以整理成 matrix，直接矩阵矢量乘法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c2a487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 | 3 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b45688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 | 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54496021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough, \n",
    "    RunnableLambda, \n",
    "    RunnableParallel\n",
    ")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'lcel_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff17e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone go to therapy? Because it had too many toppings!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# lcel\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4752657",
   "metadata": {},
   "source": [
    "### Runables\n",
    "- `RunnablePassthrough()`: identity\n",
    "- `RunnableLambda()`: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc50b5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough() | RunnablePassthrough () | RunnablePassthrough ()\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa298ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(lambda x: x.upper())\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c689e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(lambda x: x.upper()) | RunnablePassthrough()\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d5cb7",
   "metadata": {},
   "source": [
    "### json test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8125247",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'json_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d283e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d68cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", \n",
    "                 model_kwargs={'response_format': {\"type\": \"json_object\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d117ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e26f24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''I want you to extract the person name, age and a description from the following text.\n",
    "    Here is the JSON object, output:\n",
    "    {{\n",
    "        \"name\": string,\n",
    "        \"age\": int,\n",
    "        \"description\": string\n",
    "    }}'''),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 创建 LCEL 链\n",
    "chain = (\n",
    "    {\"input\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm \n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0a35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6c2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}') additional_kwargs={}\n",
      "prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}') additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(prompt[0])\n",
    "print(prompt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced032c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [SystemMessagePromptTemplate.from_template('''I want you to extract the person name, age and a description from the following text.\n",
    "    Here is the JSON object, output:\n",
    "    {{\n",
    "        \"name\": string,\n",
    "        \"age\": int,\n",
    "        \"description\": string\n",
    "    }}'''), \n",
    "     HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9090caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"John is 20 years old. He is a student at the University of California, Berkeley. He is a very smart student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "242f5c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John',\n",
       " 'age': 20,\n",
       " 'description': 'He is a student at the University of California, Berkeley. He is a very smart student.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f8c930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John',\n",
       " 'age': 20,\n",
       " 'description': 'He is a student at the University of California, Berkeley. He is a very smart student.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"John is 20 years old. He is a student at the University of California, Berkeley. He is a very smart student.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0909",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b175fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'rag_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e221556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86701928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000016320049F90>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000016320049E70>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cafeea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([\"Cats love thuna\"], embedding = OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbe6c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56406929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b6f9cf5a-8e5b-48db-9870-ae34b6d7f59f', metadata={}, page_content='Cats love thuna')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What do cats like to eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f5540ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "278c12aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuna'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What do cats like to eat?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec9681",
   "metadata": {},
   "source": [
    "### Tool uses\n",
    "- precise math calculation\n",
    "- custom tools (自定义 functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccf90a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'tools_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e78229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47e2845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(num1: float, num2: float) -> float:\n",
    "    \"Add two numbers.\"\n",
    "    return num1 + num2\n",
    "    \n",
    "@tool\n",
    "def subtract(num1: float, num2: float) -> float:\n",
    "    \"\"\"\n",
    "    Subtract two numbers.\n",
    "    \"\"\"\n",
    "    return num1 - num2\n",
    "    \n",
    "@tool\n",
    "def multiply(num1: float, num2: float) -> float:\n",
    "    \"\"\"Multiply two float .\"\"\"\n",
    "    return num1 * num2\n",
    "\n",
    "@tool\n",
    "def divide(numerator: float, denominator: float) -> float:\n",
    "    \"\"\"\n",
    "    Divides the numerator by the denominator.\n",
    "    \"\"\"\n",
    "\n",
    "    result = numerator / denominator\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def power(base: float, exponent: float) -> float:\n",
    "    \"Take the base to the exponent power, base^exponent.\"\n",
    "    return base**exponent\n",
    "\n",
    "\n",
    "@tool\n",
    "def exp(x):\n",
    "    \"\"\"\n",
    "    Calculate the natural exponential $e^x$\n",
    "    \"\"\"\n",
    "    return np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "050cb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3d8dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, divide, power, exp]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True)\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a helpful math assistant that uses calculation functions to solve complex math problems step by step.\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{input}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        HumanMessagePromptTemplate.from_template(input_variables=[\"input\"], template=human_template),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a59f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60bf855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exp` with `{'x': -2.5}`\n",
      "responded: The derivative of the sigmoid function, often denoted as \\(\\sigma(x)\\), is given by the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
      "\\]\n",
      "\n",
      "where \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\).\n",
      "\n",
      "To find the derivative of the sigmoid function at \\(x = 2.5\\), we need to:\n",
      "\n",
      "1. Calculate \\(\\sigma(2.5)\\).\n",
      "2. Use the derivative formula \\(\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\\).\n",
      "\n",
      "Let's start by calculating \\(\\sigma(2.5)\\):\n",
      "\n",
      "\\[\n",
      "\\sigma(2.5) = \\frac{1}{1 + e^{-2.5}}\n",
      "\\]\n",
      "\n",
      "I'll calculate \\(e^{-2.5}\\) first.\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m0.0820849986238988\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'num1': 1, 'num2': 0.0820849986238988}`\n",
      "responded: The value of \\(e^{-2.5}\\) is approximately \\(0.0821\\).\n",
      "\n",
      "Now, we can calculate \\(\\sigma(2.5)\\):\n",
      "\n",
      "\\[\n",
      "\\sigma(2.5) = \\frac{1}{1 + 0.0821}\n",
      "\\]\n",
      "\n",
      "Let's compute this value.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m1.0820849986238987\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `divide` with `{'numerator': 1, 'denominator': 1.0820849986238987}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m0.9241418199787566\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `subtract` with `{'num1': 1, 'num2': 0.9241418199787566}`\n",
      "responded: The value of \\(\\sigma(2.5)\\) is approximately \\(0.9241\\).\n",
      "\n",
      "Now, let's calculate the derivative \\(\\sigma'(2.5)\\) using the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = \\sigma(2.5) \\cdot (1 - \\sigma(2.5))\n",
      "\\]\n",
      "\n",
      "Substituting the value we found:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = 0.9241 \\cdot (1 - 0.9241)\n",
      "\\]\n",
      "\n",
      "Let's compute this.\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m0.07585818002124345\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'num1': 0.9241418199787566, 'num2': 0.0758581800212434}`\n",
      "responded: The value of \\(\\sigma(2.5)\\) is approximately \\(0.9241\\).\n",
      "\n",
      "Now, let's calculate the derivative \\(\\sigma'(2.5)\\) using the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = \\sigma(2.5) \\cdot (1 - \\sigma(2.5))\n",
      "\\]\n",
      "\n",
      "Substituting the value we found:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = 0.9241 \\cdot (1 - 0.9241)\n",
      "\\]\n",
      "\n",
      "Let's compute this.\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m0.07010371654510802\u001b[0m\u001b[32;1m\u001b[1;3mThe derivative of the sigmoid function at \\(x = 2.5\\), \\(\\sigma'(2.5)\\), is approximately \\(0.0701\\).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of directive of sigmoid(2.5)?',\n",
       " 'output': \"The derivative of the sigmoid function at \\\\(x = 2.5\\\\), \\\\(\\\\sigma'(2.5)\\\\), is approximately \\\\(0.0701\\\\).\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the result of directive of sigmoid(2.5)?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98cc5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dc066a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0701])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.tensor([2.5])) * (1-F.sigmoid(torch.tensor([2.5])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
